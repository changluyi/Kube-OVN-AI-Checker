#!/usr/bin/env python3
"""
æµ‹è¯•å…³é”®è¯åŒ¹é…åŠŸèƒ½çš„ç®€å•è„šæœ¬
"""

import sys
from kube_ovn_checker.analyzers.llm_agent_analyzer import LLMAgentAnalyzer


def test_keyword_extraction():
    """æµ‹è¯•1: å…³é”®è¯æå–"""
    print("=" * 60)
    print("æµ‹è¯•1: å…³é”®è¯æå–")
    print("=" * 60)

    try:
        analyzer = LLMAgentAnalyzer()

        # æµ‹è¯•æå– network-connectivity.md çš„å…³é”®è¯
        from pathlib import Path
        doc = Path("kube_ovn_checker/knowledge/workflows/network-connectivity.md")
        keywords = analyzer._extract_search_keywords(doc)

        print(f"\nâœ… å…³é”®è¯æå–æˆåŠŸ:")
        print(f"   æ–‡æ¡£: {doc.name}")
        print(f"   å…³é”®è¯: {keywords}")

        # éªŒè¯å…³é”®è¯
        expected_keywords = ["ç½‘ç»œ", "ping", "è¿é€š", "è¿æ¥", "ä¸é€š", "timeout", "æ— æ³•è®¿é—®", "é€šä¿¡"]
        for kw in expected_keywords:
            assert kw in keywords, f"âŒ ç¼ºå°‘å…³é”®è¯: {kw}"

        print(f"\nâœ… æ‰€æœ‰é¢„æœŸå…³é”®è¯éƒ½å­˜åœ¨")

        return True
    except Exception as e:
        print(f"âŒ å…³é”®è¯æå–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_keyword_matching():
    """æµ‹è¯•2: å…³é”®è¯åŒ¹é…"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: å…³é”®è¯åŒ¹é…")
    print("=" * 60)

    try:
        from pathlib import Path
        analyzer = LLMAgentAnalyzer()

        # æµ‹è¯•åœºæ™¯
        test_cases = [
            {
                "query": "Podé—´ç½‘ç»œä¸é€š",
                "expected_contains": ["network-connectivity.md"],
                "description": "ç½‘ç»œé—®é¢˜"
            },
            {
                "query": "IPåœ°å€è€—å°½ï¼Œæ— æ³•åˆ†é…",
                "expected_contains": ["ip-management.md"],
                "description": "IPç®¡ç†é—®é¢˜"
            },
            {
                "query": "Podä¸€ç›´ContainerCreating",
                "expected_contains": ["ip-management.md"],
                "description": "Podå¯åŠ¨é—®é¢˜ï¼ˆä¸IPç›¸å…³ï¼‰"
            },
            {
                "query": "æœªçŸ¥é—®é¢˜",
                "expected_contains": ["general.md"],
                "description": "é€šç”¨é—®é¢˜"
            }
        ]

        all_passed = True
        for i, case in enumerate(test_cases, 1):
            query = case["query"]
            expected = case["expected_contains"]
            desc = case["description"]

            print(f"\næµ‹è¯• {i}: {desc}")
            print(f"  æŸ¥è¯¢: {query}")

            matched = analyzer._match_knowledge_docs(query)

            print(f"  åŒ¹é…çš„æ–‡æ¡£: {[Path(doc).name for doc in matched]}")

            # éªŒè¯
            for exp in expected:
                if not any(exp in doc for doc in matched):
                    print(f"  âŒ åº”è¯¥åŒ¹é… {exp}")
                    all_passed = False
                else:
                    print(f"  âœ… æ­£ç¡®åŒ¹é… {exp}")

        if all_passed:
            print(f"\nâœ… æ‰€æœ‰å…³é”®è¯åŒ¹é…æµ‹è¯•é€šè¿‡")
        else:
            print(f"\nâš ï¸  æœ‰éƒ¨åˆ†æµ‹è¯•å¤±è´¥")

        return all_passed
    except Exception as e:
        print(f"âŒ å…³é”®è¯åŒ¹é…æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_workflow_loading():
    """æµ‹è¯•3: å·¥ä½œæµåŠ è½½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: å·¥ä½œæµåŠ è½½")
    print("=" * 60)

    try:
        analyzer = LLMAgentAnalyzer()

        # æµ‹è¯•åŠ è½½ç½‘ç»œè¿é€šæ€§å·¥ä½œæµ
        query = "Podé—´pingä¸é€š"
        matched_docs = analyzer._match_knowledge_docs(query)

        print(f"\næŸ¥è¯¢: {query}")
        print(f"åŒ¹é…æ–‡æ¡£æ•°: {len(matched_docs)}")

        # è¯»å–å¹¶éªŒè¯å†…å®¹
        for doc_path in matched_docs:
            from pathlib import Path
            content = Path(doc_path).read_text(encoding='utf-8')

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸå†…å®¹
            if "network-connectivity" in doc_path:
                assert "ovn-trace" in content, "âŒ åº”è¯¥åŒ…å« ovn-trace"
                assert "tcpdump" in content, "âŒ åº”è¯¥åŒ…å« tcpdump"
                print(f"  âœ… {Path(doc_path).name}: åŒ…å« ovn-trace å’Œ tcpdump")
            elif "general" in doc_path:
                assert "è¯Šæ–­æ–¹æ³•è®º" in content, "âŒ åº”è¯¥åŒ…å«è¯Šæ–­æ–¹æ³•è®º"
                print(f"  âœ… {Path(doc_path).name}: åŒ…å«è¯Šæ–­æ–¹æ³•è®º")
            else:
                print(f"  âœ… {Path(doc_path).name}: åŠ è½½æˆåŠŸ")

        print(f"\nâœ… å·¥ä½œæµåŠ è½½æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ å·¥ä½œæµåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_integration():
    """æµ‹è¯•4: é›†æˆæµ‹è¯•ï¼ˆæ¨¡æ‹Ÿ diagnoseï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: é›†æˆæµ‹è¯•")
    print("=" * 60)

    try:
        analyzer = LLMAgentAnalyzer()

        # æ¨¡æ‹Ÿ diagnose ä¸­çš„çŸ¥è¯†åŠ è½½é€»è¾‘
        user_query = "Podé—´ç½‘ç»œä¸é€šï¼Œpingè¶…æ—¶"
        t0_summary = "**æ€»ä½“çŠ¶æ€**: 1/2 ä¸ªç»„ä»¶ä¸å¥åº·"

        # 1. è·å–æ¶æ„çŸ¥è¯†
        core_knowledge = analyzer.knowledge.get_architecture()[:3000]
        print(f"\nâœ… æ¶æ„çŸ¥è¯†: {len(core_knowledge)} å­—ç¬¦")

        # 2. åŒ¹é…å·¥ä½œæµ
        workflow_docs = analyzer._match_knowledge_docs(user_query)
        print(f"âœ… åŒ¹é…å·¥ä½œæµ: {len(workflow_docs)} ä¸ªæ–‡æ¡£")

        # 3. è¯»å–å·¥ä½œæµå†…å®¹
        workflow_contents = []
        for doc_path in workflow_docs:
            from pathlib import Path
            content = Path(doc_path).read_text(encoding='utf-8')
            # ç§»é™¤ frontmatter
            lines = content.split('\n')
            start_idx = 0
            for i, line in enumerate(lines):
                if i == 0 and line.strip() == '---':
                    continue
                if line.strip() == '---' and i > 0:
                    start_idx = i + 1
                    break
            workflow_contents.append('\n'.join(lines[start_idx:]))

        workflow_knowledge = "\n\n## ç›¸å…³è¯Šæ–­å·¥ä½œæµ\n\n" + "\n\n".join(workflow_contents)
        print(f"âœ… å·¥ä½œæµçŸ¥è¯†: {len(workflow_knowledge)} å­—ç¬¦")

        # 4. ç»„åˆçŸ¥è¯†
        combined_knowledge = f"{core_knowledge}{workflow_knowledge}"
        print(f"âœ… ç»„åˆçŸ¥è¯†: {len(combined_knowledge)} å­—ç¬¦")

        # éªŒè¯
        assert "ovn-trace" in combined_knowledge, "âŒ åº”è¯¥åŒ…å« ovn-trace"
        assert "tcpdump" in combined_knowledge, "âŒ åº”è¯¥åŒ…å« tcpdump"
        assert "Kube-OVN" in combined_knowledge, "âŒ åº”è¯¥åŒ…å«æ¶æ„çŸ¥è¯†"

        print(f"\nâœ… é›†æˆæµ‹è¯•é€šè¿‡")

        return True
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "ğŸš€" * 30)
    print("å…³é”®è¯è·¯ç”±åŠŸèƒ½æµ‹è¯•å¥—ä»¶")
    print("ğŸš€" * 30 + "\n")

    tests = [
        ("å…³é”®è¯æå–æµ‹è¯•", test_keyword_extraction),
        ("å…³é”®è¯åŒ¹é…æµ‹è¯•", test_keyword_matching),
        ("å·¥ä½œæµåŠ è½½æµ‹è¯•", test_workflow_loading),
        ("é›†æˆæµ‹è¯•", test_integration),
    ]

    results = []

    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{test_name}' æ‰§è¡Œå‡ºé”™: {e}")
            results.append((test_name, False))

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status}: {test_name}")

    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å…³é”®è¯è·¯ç”±åŠŸèƒ½å·²æ­£ç¡®å®ç°ã€‚")
        return 0
    else:
        print(f"\nâš ï¸ æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())
