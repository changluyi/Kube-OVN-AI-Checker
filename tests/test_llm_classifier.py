#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯• LLM åˆ†ç±»å™¨ï¼ˆä½¿ç”¨çœŸå® Transformer ç½®ä¿¡åº¦ï¼‰

è¿è¡Œå‰è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
export OPENAI_API_KEY=your-key
export LLM_API_BASE=https://api.openai.com/v1  # å¯é€‰
"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from openai import OpenAI
import math

def test_llm_classification():
    """æµ‹è¯•çº¯ LLM åˆ†ç±»ï¼ˆæ— è§„åˆ™åŒ¹é…ï¼‰"""

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY æˆ– LLM_API_KEY ç¯å¢ƒå˜é‡")
        return

    client = OpenAI(api_key=api_key, base_url=os.getenv("LLM_API_BASE"))

    # å®šä¹‰ç±»åˆ«
    categories = [
        "general",
        "pod_to_pod",
        "pod_to_pod_cross_node",
        "pod_to_service",
        "pod_to_external"
    ]

    # ç³»ç»Ÿæç¤º
    system_prompt = f"""ä½ æ˜¯ Kube-OVN ç½‘ç»œè¯Šæ–­ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·æŸ¥è¯¢åˆ†ç±»åˆ°ä»¥ä¸‹åœºæ™¯ä¹‹ä¸€ï¼š

{', '.join(categories)}

åˆ†ç±»è§„åˆ™ï¼š
1. **general** - é—®å€™è¯­ã€å¸®åŠ©è¯·æ±‚ã€éè¯Šæ–­æŸ¥è¯¢
2. **pod_to_pod** - åŒä¸€èŠ‚ç‚¹å†…çš„ Pod é€šä¿¡é—®é¢˜
3. **pod_to_pod_cross_node** - ä¸åŒèŠ‚ç‚¹çš„ Pod é€šä¿¡é—®é¢˜
4. **pod_to_service** - Kubernetes Service è®¿é—®é—®é¢˜
5. **pod_to_external** - Pod è®¿é—®å¤–éƒ¨ç½‘ç»œçš„é—®é¢˜

åªè¿”å›ç±»åˆ«åç§°ï¼Œä¸è¦è§£é‡Šã€‚"""

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "node1 çš„ pod æ— æ³•è®¿é—® node2 çš„ pod",
        "nginx pod æ— æ³•è¿æ¥åˆ° app pod",
        "å¤–éƒ¨ç½‘ç»œä¸é€š",
        "æ— æ³•è®¿é—® service nginx-svc",
        "ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©çš„å—ï¼Ÿ",
        "ç½‘ç»œå¥½åƒæœ‰é—®é¢˜ï¼Œæœ‰ç‚¹æ…¢",
        "kube-ovn-controller Pod ä¸€ç›´é‡å¯",
        "ä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„ pod æ— æ³•é€šä¿¡"
    ]

    print("ğŸ§ª LLM åˆ†ç±»æµ‹è¯•ï¼ˆçº¯ LLMï¼Œæ— è§„åˆ™åŒ¹é…ï¼‰")
    print("=" * 70)

    for query in test_queries:
        try:
            # è°ƒç”¨ LLM
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                temperature=0.0,
                logprobs=True,
                top_logprobs=3
            )

            # æå–ç»“æœ
            category = response.choices[0].message.content.strip()
            logprobs = response.choices[0].logprobs.content

            # è®¡ç®—çœŸå®ç½®ä¿¡åº¦ï¼ˆTransformer softmax æ¦‚ç‡ï¼‰
            avg_logprob = sum(token.logprob for token in logprobs) / len(logprobs)
            confidence = math.exp(avg_logprob)

            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ“ æŸ¥è¯¢: {query}")
            print(f"ğŸ¯ åˆ†ç±»: {category}")
            print(f"ğŸ“Š ç½®ä¿¡åº¦: {confidence:.3f} (åŸºäº {len(logprobs)} ä¸ª token)")

            # æ˜¾ç¤ºå‰ 2 ä¸ª token çš„æ¦‚ç‡
            print("   Token æ¦‚ç‡:")
            for tp in logprobs[:2]:
                print(f"     '{tp.token}': {math.exp(tp.logprob):.3f}")

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")

    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_llm_classification()
