"""
LLM å¤šæ–‡æ¡£åŒ¹é…æ£€ç´¢å™¨ - ä½¿ç”¨ LLM æ™ºèƒ½åŒ¹é…ç›¸å…³æ–‡æ¡£

æ ¸å¿ƒåŠŸèƒ½ï¼š
- è‡ªåŠ¨å‘ç°æ‰€æœ‰çŸ¥è¯†æ–‡æ¡£
- æ„å»ºç²¾ç®€ç´¢å¼•ï¼ˆâ‰¤1.5K tokensï¼‰
- ä½¿ç”¨ LLM è¿”å›å¤šä¸ªç›¸å…³æ–‡æ¡£åŠç½®ä¿¡åº¦è¯„åˆ†
- æ”¯æŒç¼“å­˜ï¼ˆç›¸åŒæŸ¥è¯¢ç›´æ¥è¿”å›ï¼‰
"""

import json
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from functools import lru_cache

from langchain_openai import ChatOpenAI


class LLMMultiMatchRetriever:
    """åŸºäº LLM çš„å¤šæ–‡æ¡£åŒ¹é…æ£€ç´¢å™¨

    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨æ‰«ææ‰€æœ‰çŸ¥è¯†æ–‡æ¡£
    2. æ„å»ºç²¾ç®€ç´¢å¼•ä¾› LLM åˆ†æ
    3. LLM è¿”å›å¤šä¸ªç›¸å…³æ–‡æ¡£åŠç½®ä¿¡åº¦
    4. æ”¯æŒç¼“å­˜æœºåˆ¶
    """

    def __init__(
        self,
        knowledge_dir: str,
        llm: Optional[ChatOpenAI] = None,
        use_cache: bool = True
    ):
        """åˆå§‹åŒ–æ£€ç´¢å™¨

        Args:
            knowledge_dir: çŸ¥è¯†åº“æ ¹ç›®å½•
            llm: LLM å®ä¾‹ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åˆ›å»ºé»˜è®¤å®ä¾‹ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """
        self.knowledge_dir = Path(knowledge_dir)
        self.use_cache = use_cache

        # å¦‚æœæ²¡æœ‰ä¼ å…¥ LLMï¼Œä»ç¯å¢ƒå˜é‡åˆ›å»º
        if llm is None:
            import os
            api_key = os.getenv("LLM_API_KEY") or os.getenv("OPENAI_API_KEY")
            base_url = os.getenv("LLM_API_BASE")
            model = os.getenv("LLM_MODEL", "gpt-4o")

            if not api_key:
                raise ValueError(
                    "æœªæ‰¾åˆ° LLM_API_KEY æˆ– OPENAI_API_KEY ç¯å¢ƒå˜é‡\n"
                    "è¯·è®¾ç½®: export LLM_API_KEY='your-key'"
                )

            llm = ChatOpenAI(
                model=model,
                api_key=api_key,
                base_url=base_url,  # None ä¼šä½¿ç”¨é»˜è®¤ OpenAI ç«¯ç‚¹
                temperature=0.1,
            )
            print(f"âœ… LLM åˆå§‹åŒ–: model={model}, base_url={base_url or 'default'}")

        self.llm = llm

        # è‡ªåŠ¨å‘ç°æ‰€æœ‰æ–‡æ¡£
        from kube_ovn_checker.knowledge.retriever import MetadataRetriever
        base_retriever = MetadataRetriever(knowledge_dir)
        self._documents = base_retriever._documents

        # æ„å»ºç²¾ç®€ç´¢å¼•ï¼ˆå¯ç”¨ debugï¼‰
        import os
        debug_mode = os.getenv('DEBUG_INDEX', 'false').lower() == 'true' or os.getenv('VERBOSE', 'false').lower() == 'true'
        self._doc_index = self._build_compact_index(debug=debug_mode)

        # å†…å­˜ç¼“å­˜ {query_hash: [Document]}
        self._cache: Dict[str, List[Dict]] = {}

        print(f"âœ… LLM æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ: {len(self._documents)} ä¸ªæ–‡æ¡£")

    def _build_compact_index(self, debug: bool = False) -> str:
        """æ„å»ºç²¾ç®€çš„æ–‡æ¡£ç´¢å¼•ï¼ˆç”¨äº LLM åŒ¹é…ï¼‰

        åªä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå‡å°‘ token æ¶ˆè€—

        Args:
            debug: æ˜¯å¦æ‰“å° debug ä¿¡æ¯

        Returns:
            ç²¾ç®€ç´¢å¼•æ–‡æœ¬ï¼ˆâ‰¤1.5K tokensï¼‰
        """
        if debug:
            print("\n" + "=" * 70)
            print("ğŸ” å¼€å§‹æ„å»ºæ–‡æ¡£ç´¢å¼•...")
            print("=" * 70)

        lines = ["## çŸ¥è¯†åº“æ–‡æ¡£ç´¢å¼•\n"]

        # æŒ‰åˆ†ç±»åˆ†ç»„
        by_category = {}
        for doc in self._documents:
            cat = doc.category
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(doc)

        if debug:
            print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡: {len(by_category)} ä¸ªåˆ†ç±»")
            for cat, docs in sorted(by_category.items()):
                print(f"  - {cat}: {len(docs)} ä¸ªæ–‡æ¡£")

        # ç”Ÿæˆç´¢å¼•
        for category, docs in sorted(by_category.items()):
            lines.append(f"\n### {category.upper()}")

            # æŒ‰ä¼˜å…ˆçº§æ’åº
            sorted_docs = sorted(docs, key=lambda d: d.priority)

            if debug:
                print(f"\nğŸ“ åˆ†ç±» [{category.upper()}] ({len(sorted_docs)} ä¸ªæ–‡æ¡£):")

            for i, doc in enumerate(sorted_docs, 1):
                # ç²¾ç®€æ ¼å¼ï¼šåªä¿ç•™è·¯å¾„ã€æ ‡é¢˜ã€è§¦å‘è¯
                # ç¡®ä¿ triggers éƒ½æ˜¯å­—ç¬¦ä¸²
                # åŒ…å«æ‰€æœ‰ triggers,ä¸é™åˆ¶æ•°é‡(æé«˜åŒ¹é…ç²¾åº¦)
                triggers_str = ', '.join(str(t) for t in doc.triggers) if doc.triggers else 'æ— '
                lines.append(f"{i}. **{doc.title}**")
                lines.append(f"   - è·¯å¾„: `{doc.path}`")
                lines.append(f"   - è§¦å‘è¯: {triggers_str}")

                if debug:
                    print(f"  {i}. {doc.title}")
                    print(f"     è·¯å¾„: {doc.path}")
                    print(f"     è§¦å‘è¯: {triggers_str}")
                    print(f"     ä¼˜å…ˆçº§: {doc.priority}, Tokens: {doc.estimated_tokens}")

        index = "\n".join(lines)

        # éªŒè¯å¤§å°
        estimated_tokens = len(index) // 4  # ç²—ç•¥ä¼°ç®—
        char_count = len(index)

        print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆ:")
        print(f"   - å­—ç¬¦æ•°: {char_count}")
        print(f"   - ä¼°ç®— tokens: ~{estimated_tokens}")
        print(f"   - ç›®æ ‡: <1500 tokens {'âœ… è¾¾æ ‡' if estimated_tokens < 1500 else 'âŒ è¶…æ ‡'}")

        if debug:
            print("\n" + "=" * 70)
            print("ğŸ“„ å®Œæ•´ç´¢å¼•å†…å®¹:")
            print("=" * 70)
            print(index)
            print("=" * 70)

        return index

    def _llm_match_documents(self, query: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ LLM åŒ¹é…æ–‡æ¡£

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            åŒ¹é…ç»“æœåˆ—è¡¨:
            [
                {"path": "pod-communication/cross-node-overlay.md",
                 "confidence": 0.95,
                 "reason": "æ˜ç¡®æåˆ°è·¨èŠ‚ç‚¹ overlay"},
                {"path": "pod-communication/mtu-configuration.md",
                 "confidence": 0.70,
                 "reason": "æåˆ° MTU åˆ†ç‰‡é—®é¢˜"}
            ]
        """
        prompt = f"""ä½ æ˜¯ Kube-OVN çŸ¥è¯†åº“åŒ¹é…ä¸“å®¶ã€‚

## ç”¨æˆ·æŸ¥è¯¢

{query}

## çŸ¥è¯†åº“æ–‡æ¡£ç´¢å¼•

{self._doc_index}

## ä»»åŠ¡

åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›æ‰€æœ‰ç›¸å…³çš„æ–‡æ¡£è·¯å¾„ã€‚

## è¾“å‡ºæ ¼å¼

ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„:

```json
[
  {{"path": "pod-communication/cross-node-overlay.md", "confidence": 0.95, "reason": "æ˜ç¡®æåˆ°è·¨èŠ‚ç‚¹ overlay"}},
  {{"path": "pod-communication/mtu-configuration.md", "confidence": 0.70, "reason": "æåˆ° MTU åˆ†ç‰‡é—®é¢˜"}}
]
```

## ç½®ä¿¡åº¦æ ‡å‡†

- **0.9-1.0**: éå¸¸ç¡®å®šï¼ˆå…³é”®è¯æ˜ç¡®åŒ¹é…ï¼‰
- **0.7-0.9**: æ¯”è¾ƒç¡®å®šï¼ˆåœºæ™¯ç›¸å…³ï¼‰
- **0.5-0.7**: å¯èƒ½ç›¸å…³ï¼ˆæœ‰å‚è€ƒä»·å€¼ï¼‰
- **<0.5**: ä¸ç›¸å…³ï¼ˆä¸è¦è¿”å›ï¼‰

## é‡è¦æç¤º

- path å¿…é¡»å®Œå…¨åŒ¹é…ç´¢å¼•ä¸­çš„è·¯å¾„
- confidence å¿…é¡»åœ¨ 0-1 ä¹‹é—´
- reason ç”¨ä¸­æ–‡ç®€è¿°åŒ¹é…ç†ç”±
"""

        try:
            # è°ƒç”¨ LLM
            response = self.llm.invoke(prompt)
            content = response.content

            # æå– JSON
            json_match = re.search(r'```json\n(.*?)\n```', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£æ
                json_str = content.strip()

            matches = json.loads(json_str)

            # éªŒè¯è·¯å¾„
            valid_matches = []
            for match in matches:
                if self._find_doc_by_path(match["path"]):
                    valid_matches.append(match)
                else:
                    print(f"âš ï¸  LLM è¿”å›çš„è·¯å¾„ä¸å­˜åœ¨: {match['path']}")

            return valid_matches

        except Exception as e:
            print(f"âŒ LLM åŒ¹é…å¤±è´¥: {e}")
            return []

    def _find_doc_by_path(self, path: str) -> Optional[Any]:
        """æ ¹æ®è·¯å¾„æŸ¥æ‰¾æ–‡æ¡£

        Args:
            path: æ–‡æ¡£è·¯å¾„

        Returns:
            Document å¯¹è±¡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        for doc in self._documents:
            if doc.path == path:
                return doc
        return None

    def _generate_cache_key(self, query: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            MD5 å“ˆå¸Œå€¼
        """
        return hashlib.md5(query.encode()).hexdigest()

    def retrieve(
        self,
        query: str,
        max_tokens: int = 10000
    ) -> List[Any]:
        """æ™ºèƒ½æ£€ç´¢æ–‡æ¡£

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            max_tokens: æœ€å¤§ Token æ•°é‡é™åˆ¶

        Returns:
            æŒ‰ç½®ä¿¡åº¦æ’åºçš„æ–‡æ¡£åˆ—è¡¨
        """
        # 1. æ£€æŸ¥ç¼“å­˜
        if self.use_cache:
            cache_key = self._generate_cache_key(query)
            if cache_key in self._cache:
                print(f"âœ… ç¼“å­˜å‘½ä¸­: {query}")
                cached_paths = self._cache[cache_key]
                return [self._find_doc_by_path(p["path"]) for p in cached_paths if self._find_doc_by_path(p["path"])]

        # 2. LLM åŒ¹é…å¤šæ–‡æ¡£
        print(f"ğŸ” LLM åŒ¹é…: {query}")
        matches = self._llm_match_documents(query)

        if not matches:
            # ä¸é™çº§ï¼šæŠ›å‡ºå¼‚å¸¸ï¼Œè¦æ±‚ LLM å¿…é¡»å·¥ä½œ
            raise RuntimeError(
                f"LLM åŒ¹é…å¤±è´¥ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚æŸ¥è¯¢: {query}\n"
                f"è¯·æ£€æŸ¥: 1) OPENAI_API_KEY æ˜¯å¦é…ç½® 2) ç½‘ç»œæ˜¯å¦èƒ½è®¿é—® OpenAI API"
            )

        # 3. æ„å»ºç»“æœï¼ˆæŒ‰ confidence æ’åºï¼‰
        matched_docs = []
        for match in matches:
            doc = self._find_doc_by_path(match["path"])
            if doc:
                matched_docs.append((doc, match["confidence"], match["reason"]))
                print(f"  âœ… {doc.title} (ç½®ä¿¡åº¦: {match['confidence']:.2f}, ç†ç”±: {match['reason']})")

        matched_docs.sort(key=lambda x: x[1], reverse=True)

        # 4. Token é™åˆ¶
        result = []
        total_tokens = 0

        for doc, confidence, reason in matched_docs:
            if total_tokens + doc.estimated_tokens <= max_tokens:
                result.append(doc)
                total_tokens += doc.estimated_tokens
            else:
                # å°è¯•æˆªæ–­æ–‡æ¡£ä»¥é€‚åº”å‰©ä½™ç©ºé—´
                remaining_tokens = max_tokens - total_tokens
                if remaining_tokens > 500:  # è‡³å°‘ä¿ç•™ 500 tokens
                    print(f"  âš ï¸  æˆªæ–­æ–‡æ¡£: {doc.title}")
                    # TODO: å®ç°æˆªæ–­é€»è¾‘
                break

        # 5. ç¼“å­˜ç»“æœ
        if self.use_cache:
            self._cache[cache_key] = matches

        print(f"âœ… è¿”å› {len(result)} ä¸ªæ–‡æ¡£ï¼Œæ€»è®¡ ~{total_tokens} tokens")
        return result


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv

    load_dotenv()

    # æµ‹è¯• LLM æ£€ç´¢å™¨
    retriever = LLMMultiMatchRetriever(
        knowledge_dir="kube_ovn_checker/knowledge"
    )

    print("\n" + "=" * 70)
    print("ğŸ§ª LLM å¤šæ–‡æ¡£åŒ¹é…æµ‹è¯•")
    print("=" * 70)

    # æµ‹è¯•1: å•åœºæ™¯åŒ¹é…
    print("\nğŸ“‹ æµ‹è¯•1: å•åœºæ™¯åŒ¹é…")
    query1 = "è·¨èŠ‚ç‚¹ overlay é€šä¿¡å¤±è´¥"
    docs1 = retriever.retrieve(query1)
    print(f"æŸ¥è¯¢: {query1}")
    print(f"ç»“æœ: {len(docs1)} ä¸ªæ–‡æ¡£")

    # æµ‹è¯•2: å¤šåœºæ™¯åŒ¹é…
    print("\nğŸ“‹ æµ‹è¯•2: å¤šåœºæ™¯åŒ¹é…")
    query2 = "è·¨èŠ‚ç‚¹ overlay é€šä¿¡å¤±è´¥ï¼Œè¿˜æœ‰ MTU åˆ†ç‰‡é—®é¢˜"
    docs2 = retriever.retrieve(query2)
    print(f"æŸ¥è¯¢: {query2}")
    print(f"ç»“æœ: {len(docs2)} ä¸ªæ–‡æ¡£")

    # æµ‹è¯•3: ç¼“å­˜æµ‹è¯•
    print("\nğŸ“‹ æµ‹è¯•3: ç¼“å­˜æµ‹è¯•")
    docs3 = retriever.retrieve(query1)  # åº”è¯¥å‘½ä¸­ç¼“å­˜
    print(f"æŸ¥è¯¢: {query1} (ç¼“å­˜)")
    print(f"ç»“æœ: {len(docs3)} ä¸ªæ–‡æ¡£")
