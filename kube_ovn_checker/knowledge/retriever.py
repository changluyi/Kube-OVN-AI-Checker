"""
çŸ¥è¯†æ£€ç´¢å™¨ - åŸºäºå…ƒæ•°æ®çš„çŸ¥è¯†æ–‡æ¡£æ£€ç´¢

æ ¸å¿ƒåŠŸèƒ½ï¼š
- æ‰«æçŸ¥è¯†åº“ç›®å½•ï¼Œè§£ææ–‡æ¡£ YAML frontmatter
- åŸºäºåˆ†ç±»å’Œè§¦å‘è¯åŒ¹é…æ£€ç´¢ç›¸å…³æ–‡æ¡£
- æ”¯æŒä¼˜å…ˆçº§æ’åºå’Œ Token æ•°é‡é™åˆ¶
"""

import re
from pathlib import Path
from typing import List, Dict, Any, Optional
import yaml


class Document:
    """çŸ¥è¯†æ–‡æ¡£

    Attributes:
        path: æ–‡æ¡£ç›¸å¯¹è·¯å¾„
        title: æ–‡æ¡£æ ‡é¢˜
        category: æ‰€å±åˆ†ç±»
        triggers: è§¦å‘å…³é”®è¯åˆ—è¡¨
        priority: ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°è¶Šé‡è¦ï¼‰
        content: æ–‡æ¡£å†…å®¹ï¼ˆå»é™¤ frontmatterï¼‰
        estimated_tokens: ä¼°ç®—çš„ Token æ•°é‡
    """

    def __init__(
        self,
        path: str,
        title: str,
        category: str,
        triggers: List[str],
        priority: int,
        content: str,
        estimated_tokens: int
    ):
        self.path = path
        self.title = title
        self.category = category
        self.triggers = triggers
        self.priority = priority
        self.content = content
        self.estimated_tokens = estimated_tokens

    def __repr__(self):
        return f"Document(path={self.path}, category={self.category}, priority={self.priority})"


class MetadataRetriever:
    """åŸºäºå…ƒæ•°æ®çš„çŸ¥è¯†æ£€ç´¢å™¨ï¼ˆè‡ªåŠ¨å‘ç°ç‰ˆæœ¬ï¼‰

    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨æ‰«æçŸ¥è¯†åº“ç›®å½•ï¼Œè§£æ YAML frontmatter
    2. æ ¹æ®åˆ†ç±»å’Œè§¦å‘è¯æ£€ç´¢æ–‡æ¡£
    3. æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œæ§åˆ¶ Token æ•°é‡

    æ”¹è¿›ï¼š
    - ç§»é™¤ç¡¬ç¼–ç  CATEGORY_PATHS
    - è‡ªåŠ¨å‘ç°æ‰€æœ‰ .md æ–‡æ¡£
    """

    def __init__(self, knowledge_dir: Optional[str] = None):
        """åˆå§‹åŒ–æ£€ç´¢å™¨

        Args:
            knowledge_dir: çŸ¥è¯†åº“æ ¹ç›®å½•ï¼ˆé»˜è®¤ä¸º kube_ovn_checker/knowledge/ï¼‰
        """
        if knowledge_dir is None:
            # é»˜è®¤è·¯å¾„
            current_dir = Path(__file__).parent
            self.knowledge_dir = current_dir
        else:
            self.knowledge_dir = Path(knowledge_dir)

        # è‡ªåŠ¨å‘ç°æ‰€æœ‰æ–‡æ¡£ï¼ˆæ›¿ä»£ç¡¬ç¼–ç  CATEGORY_PATHSï¼‰
        self._documents = self._auto_discover_documents()

        # æ–‡æ¡£ç¼“å­˜ {category: [Document]}
        self._cache: Dict[str, List[Document]] = {}

    def _auto_discover_documents(self) -> List[Document]:
        """è‡ªåŠ¨æ‰«ææ‰€æœ‰ .md æ–‡æ¡£

        Returns:
            æ‰€æœ‰å‘ç°çš„æ–‡æ¡£åˆ—è¡¨
        """
        documents = []

        # é€’å½’æ‰«ææ‰€æœ‰ .md æ–‡ä»¶
        for md_file in self.knowledge_dir.rglob("*.md"):
            # è·³è¿‡å¤‡ä»½æ–‡ä»¶å’Œéšè—æ–‡ä»¶
            if "backup" in md_file.name or md_file.name.startswith("."):
                continue

            doc = self._load_document(md_file)
            if doc:
                documents.append(doc)

        print(f"âœ… è‡ªåŠ¨å‘ç° {len(documents)} ä¸ªçŸ¥è¯†æ–‡æ¡£")
        return documents

    def _parse_frontmatter(self, content: str) -> Dict[str, Any]:
        """è§£æ YAML frontmatter

        Args:
            content: æ–‡æ¡£å®Œæ•´å†…å®¹ï¼ˆåŒ…å« frontmatterï¼‰

        Returns:
            è§£æåçš„å…ƒæ•°æ®å­—å…¸
        """
        # æå– frontmatterï¼ˆåœ¨ --- ä¹‹é—´ï¼‰
        pattern = r'^---\n(.*?)\n---'
        match = re.match(pattern, content, re.DOTALL)

        if not match:
            return {}

        try:
            frontmatter = yaml.safe_load(match.group(1))
            return frontmatter or {}
        except yaml.YAMLError:
            return {}

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ Token æ•°é‡

        ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡çº¦ 1.5 å­— = 1 tokenï¼Œè‹±æ–‡çº¦ 4 å­— = 1 token

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            ä¼°ç®—çš„ Token æ•°é‡
        """
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # ç»Ÿè®¡éä¸­æ–‡å­—ç¬¦
        other_chars = len(text) - chinese_chars

        # ç²—ç•¥ä¼°ç®—
        tokens = chinese_chars / 1.5 + other_chars / 4
        return int(tokens)

    def _load_document(self, file_path: Path) -> Optional[Document]:
        """åŠ è½½å•ä¸ªæ–‡æ¡£

        Args:
            file_path: æ–‡æ¡£ç»å¯¹è·¯å¾„

        Returns:
            Document å¯¹è±¡ï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å› None
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # è§£æ frontmatter
            frontmatter = self._parse_frontmatter(content)

            # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€ä¸ª # æ ‡é¢˜ï¼‰
            title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            title = title_match.group(1) if title_match else file_path.stem

            # æå–å…ƒæ•°æ®
            # å…¼å®¹æ—§æ ¼å¼ï¼šsearch_keywords -> triggers
            triggers = frontmatter.get('triggers') or frontmatter.get('search_keywords') or []
            category = frontmatter.get('category', 'general')
            priority = frontmatter.get('priority', 999)  # é»˜è®¤æœ€ä½ä¼˜å…ˆçº§

            # å»é™¤ frontmatterï¼Œä¿ç•™æ­£æ–‡
            body = re.sub(r'^---\n.*?\n---\n', '', content, flags=re.DOTALL)

            # ä¼°ç®— Token æ•°é‡
            tokens = self._estimate_tokens(body)

            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            relative_path = str(file_path.relative_to(self.knowledge_dir))

            return Document(
                path=relative_path,
                title=title,
                category=category,
                triggers=triggers,
                priority=priority,
                content=body,
                estimated_tokens=tokens
            )

        except Exception as e:
            import warnings
            warnings.warn(f"Failed to load document {file_path}: {e}")
            return None

    def _scan_directory(self, category: str) -> List[Document]:
        """ä»è‡ªåŠ¨å‘ç°çš„æ–‡æ¡£ä¸­è¿‡æ»¤æŒ‡å®šåˆ†ç±»çš„æ–‡æ¡£

        Args:
            category: åˆ†ç±»åç§°ï¼ˆå¦‚ "pod_to_pod_cross_node"ï¼‰

        Returns:
            è¯¥åˆ†ç±»çš„æ–‡æ¡£åˆ—è¡¨
        """
        # æ£€æŸ¥ç¼“å­˜
        if category in self._cache:
            return self._cache[category]

        documents = []

        # ä»è‡ªåŠ¨å‘ç°çš„æ–‡æ¡£ä¸­è¿‡æ»¤
        for doc in self._documents:
            if doc.category == category:
                documents.append(doc)

        # ç¼“å­˜ç»“æœ
        self._cache[category] = documents

        return documents

    def retrieve(
        self,
        category: str,
        max_tokens: int = 10000,
        keywords: Optional[List[str]] = None
    ) -> List[Document]:
        """æ£€ç´¢çŸ¥è¯†æ–‡æ¡£

        Args:
            category: åˆ†ç±»åç§°
                - "pod_to_pod": åŒèŠ‚ç‚¹ Pod é€šä¿¡
                - "pod_to_pod_cross_node": è·¨èŠ‚ç‚¹ Pod é€šä¿¡
                - "pod_to_service": Service è®¿é—®
                - "pod_to_external": å¤–éƒ¨ç½‘ç»œè®¿é—®
            max_tokens: æœ€å¤§ Token æ•°é‡é™åˆ¶
            keywords: å¯é€‰çš„å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºè¿›ä¸€æ­¥è¿‡æ»¤æ–‡æ¡£

        Returns:
            æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆToken æ€»é‡å— max_tokens é™åˆ¶ï¼‰
        """
        # æ‰«ææ–‡æ¡£
        documents = self._scan_directory(category)

        # æŒ‰å…³é”®è¯è¿‡æ»¤ï¼ˆå¦‚æœæä¾›ï¼‰
        if keywords:
            filtered = []
            for doc in documents:
                # æ£€æŸ¥ triggers æ˜¯å¦åŒ¹é…ä»»ä¸€å…³é”®è¯
                for keyword in keywords:
                    if keyword.lower() in [t.lower() for t in doc.triggers]:
                        filtered.append(doc)
                        break
            documents = filtered

        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆæ•°å­—è¶Šå°è¶Šä¼˜å…ˆï¼‰
        documents = sorted(documents, key=lambda d: d.priority)

        # é™åˆ¶ Token æ•°é‡ï¼ˆè´ªå¿ƒç®—æ³•ï¼šä¼˜å…ˆå–é«˜ä¼˜å…ˆçº§æ–‡æ¡£ï¼‰
        result = []
        total_tokens = 0

        for doc in documents:
            if total_tokens + doc.estimated_tokens <= max_tokens:
                result.append(doc)
                total_tokens += doc.estimated_tokens
            else:
                # å°è¯•æˆªæ–­æ–‡æ¡£ä»¥é€‚åº”å‰©ä½™ç©ºé—´
                remaining_tokens = max_tokens - total_tokens
                if remaining_tokens > 500:  # è‡³å°‘ä¿ç•™ 500 tokens
                    # æˆªæ–­å†…å®¹
                    ratio = remaining_tokens / doc.estimated_tokens
                    truncated_content = doc.content[:int(len(doc.content) * ratio)]

                    # åˆ›å»ºæˆªæ–­åçš„æ–‡æ¡£å‰¯æœ¬
                    truncated_doc = Document(
                        path=doc.path,
                        title=doc.title,
                        category=doc.category,
                        triggers=doc.triggers,
                        priority=doc.priority,
                        content=truncated_content + "\n\n...(å†…å®¹å·²æˆªæ–­)",
                        estimated_tokens=remaining_tokens
                    )
                    result.append(truncated_doc)
                    total_tokens += remaining_tokens
                break

        return result

    def get_architecture_doc(self) -> Optional[Document]:
        """è·å–æ¶æ„æ–‡æ¡£ï¼ˆé«˜ä¼˜å…ˆçº§åŸºç¡€æ–‡æ¡£ï¼‰

        Returns:
            æ¶æ„æ–‡æ¡£ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        arch_path = self.knowledge_dir / "architecture.md"

        if not arch_path.exists():
            return None

        return self._load_document(arch_path)

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self._cache.clear()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import json

    retriever = MetadataRetriever()

    print("=" * 70)
    print("ğŸ§ª çŸ¥è¯†æ£€ç´¢å™¨æµ‹è¯•")
    print("=" * 70)

    # æµ‹è¯•1: è·å–æ¶æ„æ–‡æ¡£
    print("\nğŸ“‹ æµ‹è¯•1: è·å–æ¶æ„æ–‡æ¡£")
    arch_doc = retriever.get_architecture_doc()
    if arch_doc:
        print(f"  æ ‡é¢˜: {arch_doc.title}")
        print(f"  è·¯å¾„: {arch_doc.path}")
        print(f"  ä¼°ç®— Tokens: {arch_doc.estimated_tokens}")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ°æ¶æ„æ–‡æ¡£")

    # æµ‹è¯•2: æ£€ç´¢åŒèŠ‚ç‚¹ Pod é€šä¿¡ç›¸å…³æ–‡æ¡£
    print("\nğŸ“‹ æµ‹è¯•2: æ£€ç´¢åŒèŠ‚ç‚¹ Pod é€šä¿¡æ–‡æ¡£ (max_tokens=5000)")
    docs = retriever.retrieve("pod_to_pod", max_tokens=5000)

    print(f"  æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£:")
    for doc in docs:
        print(f"    - {doc.title} ({doc.estimated_tokens} tokens, priority={doc.priority})")

    # æµ‹è¯•3: æ£€ç´¢è·¨èŠ‚ç‚¹ Pod é€šä¿¡ç›¸å…³æ–‡æ¡£
    print("\nğŸ“‹ æµ‹è¯•3: æ£€ç´¢è·¨èŠ‚ç‚¹ Pod é€šä¿¡æ–‡æ¡£ (max_tokens=3000)")
    docs = retriever.retrieve("pod_to_pod_cross_node", max_tokens=3000)

    total_tokens = sum(d.estimated_tokens for d in docs)
    print(f"  æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£ (æ€»è®¡ {total_tokens} tokens):")
    for doc in docs:
        print(f"    - {doc.title} ({doc.estimated_tokens} tokens, priority={doc.priority})")

    # æµ‹è¯•4: å¸¦å…³é”®è¯è¿‡æ»¤
    print("\nğŸ“‹ æµ‹è¯•4: å¸¦å…³é”®è¯è¿‡æ»¤ (keywords=['ping', 'è¿é€š'])")
    docs = retriever.retrieve(
        "pod_to_pod",
        max_tokens=10000,
        keywords=["ping", "è¿é€š"]
    )

    print(f"  æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£:")
    for doc in docs:
        print(f"    - {doc.title}")
        print(f"      è§¦å‘è¯: {doc.triggers}")

    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆ")
